<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>Surrogate Model Assisted Hyperband Optimizer — OptimizerSumoHB • miesmuschel</title>


<!-- jquery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
<!-- Bootstrap -->

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous" />

<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script>

<!-- bootstrap-toc -->
<link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script>

<!-- Font Awesome icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous" />

<!-- clipboard.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script>

<!-- headroom.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script>

<!-- pkgdown -->
<link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script>




<meta property="og:title" content="Surrogate Model Assisted Hyperband Optimizer — OptimizerSumoHB" />
<meta property="og:description" content="Perform Surrogate Model Assisted Hyperband Optimization.
Given a population size mu, a fraction of surviving individuals survival_fraction, a number of generations n and a fidelity progression F1, F1, ..., Fn,
the algorithm works as follows:
Sample an initial design of size mu at fidelity F1
Kill individuals that are not in the top survival_fraction part of individuals, by performance
Generate new individuals using random sampling and optionally a Filtor, until mu alive individuals are present again.
Evaluate all alive individuals at fidelity F{generation}
Jump to 2., until termination, possibly because n generations are reached.


The number of generations n is determined from the OptimInstance's Terminator object, see Terminating
below.
The fidelity progression uses a specially designated &quot;budget&quot; parameter of the OptimInstance, which must have a &quot;budget&quot; tag.
The lower limit of the budget parameter is used as F0. The upper limit is used as Fn. The budget is evaluated at equally spaced values in generations 0..n,
so F1 - F0 = F2 - F1 etc. In many cases it is desirable to have a multiplicative progression of &quot;difficulty&quot; of the problem. In this case, it is
recommended to use a budget parameter with exponential &quot;trafo&quot;, or one with logscale = TRUE (see example).
A Filtor can be used for filtering, see the respective class's documentation for details on algorithms. The FiltorSurrogateProgressive can be used
for progressive surrogate model filtering. FiltorMaybe can be used for random interleaving." />




<!-- mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->



  </head>

  <body data-spy="scroll" data-target="#toc">
    <div class="container template-reference-topic">
      <header>
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">miesmuschel</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.0.0-9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../index.html">
    <span class="fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/miesmuschel.html">Get started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/mlr-org/miesmuschel/">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
      
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      

      </header>

<div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>Surrogate Model Assisted Hyperband Optimizer</h1>
    <small class="dont-index">Source: <a href='https://github.com/mlr-org/miesmuschel/blob/master/R/OptimizerSumoHB.R'><code>R/OptimizerSumoHB.R</code></a>, <a href='https://github.com/mlr-org/miesmuschel/blob/master/R/TunerSumoHB.R'><code>R/TunerSumoHB.R</code></a></small>
    <div class="hidden name"><code>OptimizerSumoHB.Rd</code></div>
    </div>

    <div class="ref-description">
    <p>Perform Surrogate Model Assisted Hyperband Optimization.</p>
<p>Given a population size <code>mu</code>, a fraction of surviving individuals <code>survival_fraction</code>, a number of generations <code>n</code> and a fidelity progression <code>F1</code>, <code>F1</code>, ..., <code>Fn</code>,
the algorithm works as follows:</p><ol>
<li><p>Sample an initial design of size <code>mu</code> at fidelity <code>F1</code></p></li>
<li><p>Kill individuals that are not in the top <code>survival_fraction</code> part of individuals, by performance</p></li>
<li><p>Generate new individuals using random sampling and optionally a <code><a href='Filtor.html'>Filtor</a></code>, until <code>mu</code> alive individuals are present again.</p></li>
<li><p>Evaluate all alive individuals at fidelity <code>F{generation}</code></p></li>
<li><p>Jump to 2., until termination, possibly because <code>n</code> generations are reached.</p></li>
</ol>

<p>The number of generations <code>n</code> is determined from the <code><a href='https://bbotk.mlr-org.com/reference/OptimInstance.html'>OptimInstance</a></code>'s <code><a href='https://bbotk.mlr-org.com/reference/Terminator.html'>Terminator</a></code> object, see <strong>Terminating</strong>
below.</p>
<p>The <strong>fidelity progression</strong> uses a specially designated "budget" parameter of the <code><a href='https://bbotk.mlr-org.com/reference/OptimInstance.html'>OptimInstance</a></code>, which must have a <code>"budget"</code> tag.
The lower limit of the budget parameter is used as <code>F0</code>. The upper limit is used as <code>Fn</code>. The budget is evaluated at equally spaced values in generations <code>0..n</code>,
so <code>F1</code> - <code>F0</code> = <code>F2</code> - <code>F1</code> etc. In many cases it is desirable to have a multiplicative progression of "difficulty" of the problem. In this case, it is
recommended to use a budget parameter with exponential "trafo", or one with <code>logscale = TRUE</code> (see example).</p>
<p>A <code><a href='Filtor.html'>Filtor</a></code> can be used for filtering, see the respective class's documentation for details on algorithms. The <code><a href='dict_filtors_surprog.html'>FiltorSurrogateProgressive</a></code> can be used
for progressive surrogate model filtering. <code><a href='dict_filtor_maybe.html'>FiltorMaybe</a></code> can be used for random interleaving.</p>
    </div>



    <h2 class="hasAnchor" id="fidelity-steps"><a class="anchor" href="#fidelity-steps"></a>Fidelity Steps</h2>

    

<p>The number of fidelity steps can be determined through the <code>fidelity_steps</code> configuration parameter, or can be determined when a
<code><a href='mlr_terminators_gens.html'>TerminatorGenerations</a></code> is used to determine the number of fidelity refinements that are being performed. For this, the  <code><a href='https://bbotk.mlr-org.com/reference/OptimInstance.html'>OptimInstance</a></code>
being optimized must contain a <code><a href='mlr_terminators_gens.html'>TerminatorGenerations</a></code>, either directly (<code>inst$terminator</code>), or indirectly through a
<code><a href='https://bbotk.mlr-org.com/reference/mlr_terminators_combo.html'>bbotk::TerminatorCombo</a></code> with <code>$any</code> set to <code>TRUE</code> (recursive <code><a href='https://bbotk.mlr-org.com/reference/mlr_terminators_combo.html'>TerminatorCombo</a></code> may also be used). When <code>fidelity_steps</code> is <code>0</code>,
the number of generations is determined from the given <code><a href='https://bbotk.mlr-org.com/reference/Terminator.html'>Terminator</a></code> object and the number of fidelity refinements
is planned according to this number. Other terminators may be present in a <code><a href='https://bbotk.mlr-org.com/reference/mlr_terminators_combo.html'>TerminatorCombo</a></code> that may
lead to finishing the tuning process earlier.</p>
<p>It is possible to continue optimization runs that quit early due to other terminators. It is not recommended to change <code>fidelity_steps</code> (or the number of generations
when <code>fidelity_steps</code> is 0) between run continuations, however, unless the fidelity bounds are also adjusted, since the continuation would then have a decrease in fidelity.</p>
    <h2 class="hasAnchor" id="configuration-parameters"><a class="anchor" href="#configuration-parameters"></a>Configuration Parameters</h2>

    

<p><code>OptimizerSumoHB</code>'s configuration parameters are the hyperparameters of the <code><a href='Filtor.html'>Filtor</a></code> given to the <code>filtor</code> construction argument, as well as:</p><ul>
<li><p><code>mu</code> :: <code><a href='https://rdrr.io/r/base/integer.html'>integer(1)</a></code><br />
Population size: Number of individuals that are sampled in the beginning, and which are re-evaluated in each fidelity step. Initialized to 2.</p></li>
<li><p><code>survival_fraction</code> :: <code><a href='https://rdrr.io/r/base/numeric.html'>numeric(1)</a></code><br />
Fraction of the population that survives at each fidelity step. The number of newly sampled individuals is (1 - <code>survival_fraction</code>) * <code>mu</code>. Initialized to 0.5.</p></li>
<li><p><code>sampling</code> :: <code>function</code><br />
Function that generates the initial population, as well as new individuals to be filtered from, as a <code><a href='https://paradox.mlr-org.com/reference/Design.html'>Design</a></code> object. The function must have
arguments <code>param_set</code> and <code>n</code> and function like <code><a href='https://paradox.mlr-org.com/reference/generate_design_random.html'>paradox::generate_design_random</a></code> or <code><a href='https://paradox.mlr-org.com/reference/generate_design_lhs.html'>paradox::generate_design_lhs</a></code>.
This is equivalent to the <code>initializer</code> parameter of <code><a href='mies_init_population.html'>mies_init_population()</a></code>, see there for more information. Initialized to
<code><a href='https://paradox.mlr-org.com/reference/generate_design_random.html'>generate_design_random()</a></code>.</p></li>
<li><p><code>fidelity_steps</code> :: <code><a href='https://rdrr.io/r/base/integer.html'>integer(1)</a></code><br />
Number of fidelity steps. When it is 0, the number is determined from the <code><a href='https://bbotk.mlr-org.com/reference/OptimInstance.html'>OptimInstance</a></code>'s <code><a href='https://bbotk.mlr-org.com/reference/Terminator.html'>Terminator</a></code>. See the
section <strong>Fidelity Steps</strong> for more details. Initialized to 0.</p></li>
<li><p><code>filter_with_max_budget</code> :: <code><a href='https://rdrr.io/r/base/logical.html'>logical(1)</a></code><br />
Whether to perform filtering with the maximum fidelity value found in the archive, as opposed the current <code>budget_survivors</code>. This has only an effect when
<code>fidelity_steps</code> is greater than 1 and some evaluations are done when the archive already contains evaluations with greater fidelity. Initialized to <code>FALSE</code>.</p></li>
</ul>

    <h2 class="hasAnchor" id="see-also"><a class="anchor" href="#see-also"></a>See also</h2>

    <div class='dont-index'><p>Other optimizers: 
<code><a href='OptimizerMies.html'>OptimizerMies</a></code></p></div>
    <h2 class="hasAnchor" id="super-class"><a class="anchor" href="#super-class"></a>Super class</h2>

    <p><code><a href='https://bbotk.mlr-org.com/reference/Optimizer.html'>bbotk::Optimizer</a></code> -&gt; <code>OptimizerSumoHB</code></p>
    <h2 class="hasAnchor" id="active-bindings"><a class="anchor" href="#active-bindings"></a>Active bindings</h2>

    <p><div class="r6-active-bindings"></p><dl>
<dt><code>filtor</code></dt><dd><p>(<code><a href='Filtor.html'>Filtor</a></code>)<br />
Filtering algorithm used.</p></dd>

<dt><code>param_set</code></dt><dd><p>(<code><a href='https://paradox.mlr-org.com/reference/ParamSet.html'>ParamSet</a></code>)<br />
Configuration parameters of the optimization algorithm.</p></dd>

</dl><p></div></p>
    <h2 class="hasAnchor" id="methods"><a class="anchor" href="#methods"></a>Methods</h2>

    
<h3 class='hasAnchor' id='arguments'><a class='anchor' href='#arguments'></a>Public methods</h3>

<ul>
<li><p><a href='#method-new'><code>OptimizerSumoHB$new()</code></a></p></li>
<li><p><a href='#method-clone'><code>OptimizerSumoHB$clone()</code></a></p></li>
</ul>
<p><details open ><summary>Inherited methods</summary>
<ul>
<li><p><span class="pkg-link" data-pkg="bbotk" data-topic="Optimizer" data-id="format"><a href='../../bbotk/html/Optimizer.html#method-format'><code>bbotk::Optimizer$format()</code></a></span></p></li>
<li><p><span class="pkg-link" data-pkg="bbotk" data-topic="Optimizer" data-id="optimize"><a href='../../bbotk/html/Optimizer.html#method-optimize'><code>bbotk::Optimizer$optimize()</code></a></span></p></li>
<li><p><span class="pkg-link" data-pkg="bbotk" data-topic="Optimizer" data-id="print"><a href='../../bbotk/html/Optimizer.html#method-print'><code>bbotk::Optimizer$print()</code></a></span></p></li>
</ul>
</details>

<hr>
<a id="method-new"></a></p><h3 class='hasAnchor' id='arguments'><a class='anchor' href='#arguments'></a>Method <code>new()</code></h3>
<p>Initialize the 'OptimizerSumoHB' object.</p><h4 class='hasAnchor' id='arguments'><a class='anchor' href='#arguments'></a>Usage</h4>
<p><div class="r"></p><pre><span class='va'>OptimizerSumoHB</span><span class='op'>$</span><span class='fu'>new</span><span class='op'>(</span>filtor <span class='op'>=</span> <span class='va'><a href='dict_filtors_proxy.html'>FiltorProxy</a></span><span class='op'>$</span><span class='fu'>new</span><span class='op'>(</span><span class='op'>)</span><span class='op'>)</span></pre><p></div></p>

<h4 class='hasAnchor' id='arguments'><a class='anchor' href='#arguments'></a>Arguments</h4>
<p><div class="arguments"></p><dl>
<dt><code>filtor</code></dt><dd><p>(<code><a href='Filtor.html'>Filtor</a></code>)<br />
<code><a href='Filtor.html'>Filtor</a></code> for the filtering algorithm. Default is <code><a href='dict_filtors_proxy.html'>FiltorProxy</a></code>, which exposes the operation as
a configuration parameter of the optimizer itself.<br />
The <code>$filtor</code> field will reflect this value.</p></dd>

</dl><p></div></p>
<p><hr>
<a id="method-clone"></a></p><h3 class='hasAnchor' id='arguments'><a class='anchor' href='#arguments'></a>Method <code>clone()</code></h3>
<p>The objects of this class are cloneable with this method.</p><h4 class='hasAnchor' id='arguments'><a class='anchor' href='#arguments'></a>Usage</h4>
<p><div class="r"></p><pre><span class='va'>OptimizerSumoHB</span><span class='op'>$</span><span class='fu'>clone</span><span class='op'>(</span>deep <span class='op'>=</span> <span class='cn'>FALSE</span><span class='op'>)</span></pre><p></div></p>

<h4 class='hasAnchor' id='arguments'><a class='anchor' href='#arguments'></a>Arguments</h4>
<p><div class="arguments"></p><dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p></dd>

</dl><p></div></p>


    <h2 class="hasAnchor" id="super-classes"><a class="anchor" href="#super-classes"></a>Super classes</h2>

    <p><code><a href='https://mlr3tuning.mlr-org.com/reference/Tuner.html'>mlr3tuning::Tuner</a></code> -&gt; <code><a href='https://mlr3tuning.mlr-org.com/reference/TunerFromOptimizer.html'>mlr3tuning::TunerFromOptimizer</a></code> -&gt; <code>TunerSumoHB</code></p>
    <h2 class="hasAnchor" id="methods"><a class="anchor" href="#methods"></a>Methods</h2>

    
<h3 class='hasAnchor' id='arguments'><a class='anchor' href='#arguments'></a>Public methods</h3>

<ul>
<li><p><a href='#method-new'><code>TunerSumoHB$new()</code></a></p></li>
<li><p><a href='#method-clone'><code>TunerSumoHB$clone()</code></a></p></li>
</ul>
<p><details open ><summary>Inherited methods</summary>
<ul>
<li><p><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format"><a href='../../mlr3tuning/html/Tuner.html#method-format'><code>mlr3tuning::Tuner$format()</code></a></span></p></li>
<li><p><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print"><a href='../../mlr3tuning/html/Tuner.html#method-print'><code>mlr3tuning::Tuner$print()</code></a></span></p></li>
<li><p><span class="pkg-link" data-pkg="mlr3tuning" data-topic="TunerFromOptimizer" data-id="optimize"><a href='../../mlr3tuning/html/TunerFromOptimizer.html#method-optimize'><code>mlr3tuning::TunerFromOptimizer$optimize()</code></a></span></p></li>
</ul>
</details>

<hr>
<a id="method-new"></a></p><h3 class='hasAnchor' id='arguments'><a class='anchor' href='#arguments'></a>Method <code>new()</code></h3>
<p>Initialize the <code>TunerSumoHB</code> object.</p><h4 class='hasAnchor' id='arguments'><a class='anchor' href='#arguments'></a>Usage</h4>
<p><div class="r"></p><pre><span class='va'>TunerSumoHB</span><span class='op'>$</span><span class='fu'>new</span><span class='op'>(</span>filtor <span class='op'>=</span> <span class='va'><a href='dict_filtors_proxy.html'>FiltorProxy</a></span><span class='op'>$</span><span class='fu'>new</span><span class='op'>(</span><span class='op'>)</span><span class='op'>)</span></pre><p></div></p>

<h4 class='hasAnchor' id='arguments'><a class='anchor' href='#arguments'></a>Arguments</h4>
<p><div class="arguments"></p><dl>
<dt><code>surrogate_learner</code></dt><dd><p>(<code><a href='https://mlr3.mlr-org.com/reference/LearnerRegr.html'>mlr3::LearnerRegr</a></code> | <code>NULL</code>)</p></dd>

</dl><p></div></p>
<p><hr>
<a id="method-clone"></a></p><h3 class='hasAnchor' id='arguments'><a class='anchor' href='#arguments'></a>Method <code>clone()</code></h3>
<p>The objects of this class are cloneable with this method.</p><h4 class='hasAnchor' id='arguments'><a class='anchor' href='#arguments'></a>Usage</h4>
<p><div class="r"></p><pre><span class='va'>TunerSumoHB</span><span class='op'>$</span><span class='fu'>clone</span><span class='op'>(</span>deep <span class='op'>=</span> <span class='cn'>FALSE</span><span class='op'>)</span></pre><p></div></p>

<h4 class='hasAnchor' id='arguments'><a class='anchor' href='#arguments'></a>Arguments</h4>
<p><div class="arguments"></p><dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p></dd>

</dl><p></div></p>



    <h2 class="hasAnchor" id="examples"><a class="anchor" href="#examples"></a>Examples</h2>
    <pre class="examples"><div class='input'><span class='co'># \donttest{</span>
<span class='fu'>lgr</span><span class='fu'>::</span><span class='fu'><a href='https://s-fleck.github.io/lgr/reference/simple_logging.html'>threshold</a></span><span class='op'>(</span><span class='st'>"warn"</span><span class='op'>)</span>

<span class='co'>#####</span>
<span class='co'># Optimizing a Function</span>
<span class='co'>#####</span>

<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='st'><a href='https://bbotk.mlr-org.com'>"bbotk"</a></span><span class='op'>)</span>

<span class='co'># Define the objective to optimize</span>
<span class='co'># The 'budget' here simulates averaging 'b' samples from a noisy function</span>
<span class='va'>objective</span> <span class='op'>&lt;-</span> <span class='va'><a href='https://bbotk.mlr-org.com/reference/ObjectiveRFun.html'>ObjectiveRFun</a></span><span class='op'>$</span><span class='fu'>new</span><span class='op'>(</span>
  fun <span class='op'>=</span> <span class='kw'>function</span><span class='op'>(</span><span class='va'>xs</span><span class='op'>)</span> <span class='op'>{</span>
    <span class='va'>z</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/base/Log.html'>exp</a></span><span class='op'>(</span><span class='op'>-</span><span class='va'>xs</span><span class='op'>$</span><span class='va'>x</span><span class='op'>^</span><span class='fl'>2</span> <span class='op'>-</span> <span class='va'>xs</span><span class='op'>$</span><span class='va'>y</span><span class='op'>^</span><span class='fl'>2</span><span class='op'>)</span> <span class='op'>+</span> <span class='fl'>2</span> <span class='op'>*</span> <span class='fu'><a href='https://rdrr.io/r/base/Log.html'>exp</a></span><span class='op'>(</span><span class='op'>-</span><span class='op'>(</span><span class='fl'>2</span> <span class='op'>-</span> <span class='va'>xs</span><span class='op'>$</span><span class='va'>x</span><span class='op'>)</span><span class='op'>^</span><span class='fl'>2</span> <span class='op'>-</span> <span class='op'>(</span><span class='fl'>2</span> <span class='op'>-</span> <span class='va'>xs</span><span class='op'>$</span><span class='va'>y</span><span class='op'>)</span><span class='op'>^</span><span class='fl'>2</span><span class='op'>)</span>
    <span class='va'>z</span> <span class='op'>&lt;-</span> <span class='va'>z</span> <span class='op'>+</span> <span class='fu'><a href='https://rdrr.io/r/stats/Normal.html'>rnorm</a></span><span class='op'>(</span><span class='fl'>1</span>, sd <span class='op'>=</span> <span class='fl'>1</span> <span class='op'>/</span> <span class='fu'><a href='https://rdrr.io/r/base/MathFun.html'>sqrt</a></span><span class='op'>(</span><span class='va'>xs</span><span class='op'>$</span><span class='va'>b</span><span class='op'>)</span><span class='op'>)</span>
    <span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span><span class='op'>(</span>Obj <span class='op'>=</span> <span class='va'>z</span><span class='op'>)</span>
  <span class='op'>}</span>,
  domain <span class='op'>=</span> <span class='fu'><a href='https://paradox.mlr-org.com/reference/ps.html'>ps</a></span><span class='op'>(</span>x <span class='op'>=</span> <span class='fu'><a href='https://paradox.mlr-org.com/reference/Domain.html'>p_dbl</a></span><span class='op'>(</span><span class='op'>-</span><span class='fl'>2</span>, <span class='fl'>4</span><span class='op'>)</span>, y <span class='op'>=</span> <span class='fu'><a href='https://paradox.mlr-org.com/reference/Domain.html'>p_dbl</a></span><span class='op'>(</span><span class='op'>-</span><span class='fl'>2</span>, <span class='fl'>4</span><span class='op'>)</span>, b <span class='op'>=</span> <span class='fu'><a href='https://paradox.mlr-org.com/reference/Domain.html'>p_int</a></span><span class='op'>(</span><span class='fl'>1</span><span class='op'>)</span><span class='op'>)</span>,
  codomain <span class='op'>=</span> <span class='fu'><a href='https://paradox.mlr-org.com/reference/ps.html'>ps</a></span><span class='op'>(</span>Obj <span class='op'>=</span> <span class='fu'><a href='https://paradox.mlr-org.com/reference/Domain.html'>p_dbl</a></span><span class='op'>(</span>tags <span class='op'>=</span> <span class='st'>"maximize"</span><span class='op'>)</span><span class='op'>)</span>
<span class='op'>)</span>

<span class='va'>search_space</span> <span class='op'>=</span> <span class='va'>objective</span><span class='op'>$</span><span class='va'>domain</span><span class='op'>$</span><span class='fu'>search_space</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span><span class='op'>(</span>
  x <span class='op'>=</span> <span class='fu'><a href='https://paradox.mlr-org.com/reference/to_tune.html'>to_tune</a></span><span class='op'>(</span><span class='op'>)</span>,
  y <span class='op'>=</span> <span class='fu'><a href='https://paradox.mlr-org.com/reference/to_tune.html'>to_tune</a></span><span class='op'>(</span><span class='op'>)</span>,
  b <span class='op'>=</span> <span class='fu'><a href='https://paradox.mlr-org.com/reference/to_tune.html'>to_tune</a></span><span class='op'>(</span><span class='fu'><a href='https://paradox.mlr-org.com/reference/Domain.html'>p_int</a></span><span class='op'>(</span><span class='fl'>1</span>, <span class='fl'>2</span><span class='op'>^</span><span class='fl'>10</span>, logscale <span class='op'>=</span> <span class='cn'>TRUE</span>, tags <span class='op'>=</span> <span class='st'>"budget"</span><span class='op'>)</span><span class='op'>)</span>
<span class='op'>)</span><span class='op'>)</span>

<span class='co'># Get a new OptimInstance. Here we determine that the optimizatoin goes</span>
<span class='co'># for 10 generations.</span>
<span class='va'>oi</span> <span class='op'>&lt;-</span> <span class='va'><a href='https://bbotk.mlr-org.com/reference/OptimInstanceSingleCrit.html'>OptimInstanceSingleCrit</a></span><span class='op'>$</span><span class='fu'>new</span><span class='op'>(</span><span class='va'>objective</span>,
  search_space <span class='op'>=</span> <span class='va'>search_space</span>,
  terminator <span class='op'>=</span> <span class='fu'><a href='https://bbotk.mlr-org.com/reference/trm.html'>trm</a></span><span class='op'>(</span><span class='st'>"gens"</span>, generations <span class='op'>=</span> <span class='fl'>10</span><span class='op'>)</span>
<span class='op'>)</span>

<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='st'><a href='https://mlr3learners.mlr-org.com'>"mlr3learners"</a></span><span class='op'>)</span>
<span class='co'># use the 'regr.ranger' as surrogate.</span>
<span class='co'># The following settings have 30 individuals in a batch, the 20 best</span>
<span class='co'># of which survive, while 10 are sampled new.</span>
<span class='co'># For this, 100 individuals are sampled randomly, and the top 10, according</span>
<span class='co'># to the surrogate model, are used.</span>
<span class='va'>sumohb_opt</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://bbotk.mlr-org.com/reference/opt.html'>opt</a></span><span class='op'>(</span><span class='st'>"sumohb"</span>, <span class='fu'><a href='mut.html'>ftr</a></span><span class='op'>(</span><span class='st'>"surprog"</span>,
    surrogate_learner <span class='op'>=</span> <span class='fu'>mlr3</span><span class='fu'>::</span><span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>lrn</a></span><span class='op'>(</span><span class='st'>"regr.ranger"</span><span class='op'>)</span>,
    filter_pool_first <span class='op'>=</span> <span class='fl'>100</span>, filter_pool_per_sample <span class='op'>=</span> <span class='fl'>0</span><span class='op'>)</span>,
  mu <span class='op'>=</span> <span class='fl'>30</span>, survival_fraction <span class='op'>=</span> <span class='fl'>2</span><span class='op'>/</span><span class='fl'>3</span>
<span class='op'>)</span>
<span class='co'># sumohb_opt$optimize performs SumoHB optimization and returns the optimum</span>
<span class='va'>sumohb_opt</span><span class='op'>$</span><span class='fu'>optimize</span><span class='op'>(</span><span class='va'>oi</span><span class='op'>)</span>
</div><div class='output co'>#&gt;           x        y        b  x_domain      Obj
#&gt; 1: 2.301525 1.669191 1.540544 &lt;list[3]&gt; 2.868076</div><div class='input'>
<span class='co'>#####</span>
<span class='co'># Optimizing a Machine Learning Method</span>
<span class='co'>#####</span>

<span class='co'># Note that this is a short example, aiming at clarity and short runtime.</span>
<span class='co'># The settings are not optimal for hyperparameter tuning.</span>

<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='st'><a href='https://mlr3.mlr-org.com'>"mlr3"</a></span><span class='op'>)</span>
<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='st'><a href='https://mlr3learners.mlr-org.com'>"mlr3learners"</a></span><span class='op'>)</span>
<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='st'><a href='https://mlr3tuning.mlr-org.com'>"mlr3tuning"</a></span><span class='op'>)</span>

<span class='co'># The Learner to optimize</span>
<span class='va'>learner</span> <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>lrn</a></span><span class='op'>(</span><span class='st'>"classif.xgboost"</span><span class='op'>)</span>

<span class='co'># The hyperparameters to optimize</span>
<span class='va'>learner</span><span class='op'>$</span><span class='va'>param_set</span><span class='op'>$</span><span class='va'>values</span><span class='op'>[</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='st'>"eta"</span>, <span class='st'>"booster"</span><span class='op'>)</span><span class='op'>]</span> <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span><span class='op'>(</span><span class='fu'><a href='https://paradox.mlr-org.com/reference/to_tune.html'>to_tune</a></span><span class='op'>(</span><span class='op'>)</span><span class='op'>)</span>
<span class='va'>learner</span><span class='op'>$</span><span class='va'>param_set</span><span class='op'>$</span><span class='va'>values</span><span class='op'>$</span><span class='va'>nrounds</span> <span class='op'>=</span> <span class='fu'><a href='https://paradox.mlr-org.com/reference/to_tune.html'>to_tune</a></span><span class='op'>(</span><span class='fu'><a href='https://paradox.mlr-org.com/reference/Domain.html'>p_int</a></span><span class='op'>(</span><span class='fl'>1</span>, <span class='fl'>4</span>, tags <span class='op'>=</span> <span class='st'>"budget"</span>, logscale <span class='op'>=</span> <span class='cn'>TRUE</span><span class='op'>)</span><span class='op'>)</span>

<span class='co'># Get a TuningInstance</span>
<span class='va'>ti</span> <span class='op'>=</span> <span class='va'><a href='https://mlr3tuning.mlr-org.com/reference/TuningInstanceSingleCrit.html'>TuningInstanceSingleCrit</a></span><span class='op'>$</span><span class='fu'>new</span><span class='op'>(</span>
  task <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>tsk</a></span><span class='op'>(</span><span class='st'>"iris"</span><span class='op'>)</span>,
  learner <span class='op'>=</span> <span class='va'>learner</span>,
  resampling <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>rsmp</a></span><span class='op'>(</span><span class='st'>"holdout"</span><span class='op'>)</span>,
  measure <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>msr</a></span><span class='op'>(</span><span class='st'>"classif.acc"</span><span class='op'>)</span>,
  terminator <span class='op'>=</span> <span class='fu'><a href='https://bbotk.mlr-org.com/reference/trm.html'>trm</a></span><span class='op'>(</span><span class='st'>"gens"</span>, generations <span class='op'>=</span> <span class='fl'>3</span><span class='op'>)</span>
<span class='op'>)</span>

<span class='co'># use ftr("maybe") for random interleaving: only 50% of proposed points are filtered.</span>
<span class='va'>sumohb_tune</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://mlr3tuning.mlr-org.com/reference/tnr.html'>tnr</a></span><span class='op'>(</span><span class='st'>"sumohb"</span>, <span class='fu'><a href='mut.html'>ftr</a></span><span class='op'>(</span><span class='st'>"maybe"</span>, p <span class='op'>=</span> <span class='fl'>0.5</span>, filtor <span class='op'>=</span> <span class='fu'><a href='mut.html'>ftr</a></span><span class='op'>(</span><span class='st'>"surprog"</span>,
    surrogate_learner <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>lrn</a></span><span class='op'>(</span><span class='st'>"regr.ranger"</span><span class='op'>)</span>,
    filter_pool_first <span class='op'>=</span> <span class='fl'>100</span>, filter_pool_per_sample <span class='op'>=</span> <span class='fl'>0</span><span class='op'>)</span><span class='op'>)</span>,
  mu <span class='op'>=</span> <span class='fl'>20</span>, survival_fraction <span class='op'>=</span> <span class='fl'>0.5</span>
<span class='op'>)</span>
<span class='co'># sumohb_tune$optimize performs SumoHB optimization and returns the optimum</span>
<span class='va'>sumohb_tune</span><span class='op'>$</span><span class='fu'>optimize</span><span class='op'>(</span><span class='va'>ti</span><span class='op'>)</span>
</div><div class='output co'>#&gt; [04:48:33] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:33] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:33] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:33] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:33] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:33] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:33] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:33] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:33] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:33] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:33] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:33] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:33] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:33] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:33] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:33] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:33] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:33] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:33] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:33] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:35] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:35] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:35] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:35] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:35] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:35] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:35] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:35] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:35] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:35] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:35] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:36] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:36] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:36] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:36] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:36] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:36] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:36] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:36] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:36] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:38] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:38] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:38] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:38] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:38] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:38] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:38] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:38] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:38] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:38] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:38] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:38] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:38] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:38] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:38] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:38] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:38] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:38] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:38] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
#&gt; [04:48:38] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.</div><div class='output co'>#&gt;     nrounds       eta booster learner_param_vals  x_domain classif.acc
#&gt; 1: 1.609438 0.9276873    dart          &lt;list[5]&gt; &lt;list[3]&gt;        0.96</div><div class='input'>
<span class='co'># }</span>
</div></pre>
  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">
    <nav id="toc" data-toggle="toc" class="sticky-top">
      <h2 data-toc-skip>Contents</h2>
    </nav>
  </div>
</div>


      <footer>
      <div class="copyright">
  <p>Developed by Martin Binder.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
   </div>

  


  </body>
</html>


