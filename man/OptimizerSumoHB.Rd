% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/OptimizerSumoHB.R, R/TunerSumoHB.R
\name{OptimizerSumoHB}
\alias{OptimizerSumoHB}
\alias{TunerSumoHB}
\title{Surrogate Model Assisted Hyperband Optimizer}
\description{
Perform Surrogate Model Assisted Hyperband Optimization.

Given a population size \code{mu}, a fraction of surviving individuals \code{survival_fraction}, a number of generations \code{n} and a fidelity progression \code{F0}, \code{F1}, ..., \code{Fn},
the algorithm works as follows:
\enumerate{
\item Sample an initial design of size \code{mu} at fidelity \code{F0}
\item Kill individuals that are not in the top \code{survival_fraction} part of individuals, by performance
\item Generate new individuals using either random sampling, or \strong{\link[=FiltorSurrogateProgressive]{progressive surrogate model filtering}},
until \code{mu} alive individuals are present again.
\item Evaluate all alive individuals at fidelity \verb{F\{generation\}}
\item Jump to 2., until termination, possibly because \code{n} generations are reached.
}

The number of generations \code{n} is determined from the \code{\link[bbotk:OptimInstance]{OptimInstance}}'s \code{\link[bbotk:Terminator]{Terminator}} object, see \strong{Terminating}
below.

The \strong{fidelity progression} uses a specially designated "budget" parameter of the \code{\link[bbotk:OptimInstance]{OptimInstance}}, which must have a \code{"budget"} tag.
The lower limit of the budget parameter is used as \code{F0}. The upper limit is used as \code{Fn}. The budget is evaluated at equally spaced values in generations \verb{0..n},
so \code{F1} - \code{F0} = \code{F2} - \code{F1} etc. In many cases it is desirable to have a multiplicative progression of "difficulty" of the problem. In this case, it is
recommended to use a budget parameter with exponential "trafo", or one with \code{logscale = TRUE} (see example).

The \code{\link{FiltorSurrogateProgressive}} is used for filtering, see the class's documentation for details on the algorithm.
}
\section{Terminating}{

\code{\link{TerminatorGenerations}} is used to determine the number of fidelity refinements to be performed. Therefore, the  \code{\link[bbotk:OptimInstance]{OptimInstance}}
being optimized must contain a \code{\link{TerminatorGenerations}}. Either directly (\code{inst$terminator}), or indirectly through a
\code{\link[bbotk:mlr_terminators_combo]{bbotk::TerminatorCombo}} with \verb{$any} set to \code{TRUE} (recursive \code{\link[bbotk:mlr_terminators_combo]{TerminatorCombo}} may also be used). The
number of generations is determined from the given \code{\link[bbotk:Terminator]{Terminator}} object and the number of fidelity refinements
is planned according to this number. Other terminators may be present in a \code{\link[bbotk:mlr_terminators_combo]{TerminatorCombo}} that may
lead to finishing the tuning process earlier.

It is possible to continue optimization runs that quit early due to other terminators. It is not recommended to change the number of generations
between run continuations, however, unless the fidelity bounds are also adjusted, since the continuation would then have a decrease in fidelity.
}

\section{Configuration Parameters}{

\code{OptimizerSumoHB}'s configuration parameters are the hyperparameters of the \code{surrogate_learner} \code{\link[mlr3:Learner]{Learner}}, if it is not \code{NULL}, as well as:
\itemize{
\item \code{mu} :: \code{integer(1)}\cr
Population size: Number of individuals that are sampled in the beginning, and which are re-evaluated in each fidelity step. Initialized to 2.
\item \code{survival_fraction} :: \code{numeric(1)}\cr
Fraction of the population that survives at each fidelity step. The number of newly sampled individuals is (1 - \code{survival_fraction}) * \code{mu}.
\item \code{sampling} :: \code{function}\cr
Function that generates the initial population, as well as new individuals to be filtered from, as a \code{\link[paradox:Design]{Design}} object. The function must have
arguments \code{param_set} and \code{n} and function like \code{\link[paradox:generate_design_random]{paradox::generate_design_random}} or \code{\link[paradox:generate_design_lhs]{paradox::generate_design_lhs}}.
This is equivalent to the \code{initializer} parameter of \code{\link[=mies_init_population]{mies_init_population()}}, see there for more information. Initialized to
\code{\link[paradox:generate_design_random]{generate_design_random()}}.
}

The following are configuration parameters of \code{\link{FiltorSurrogateProgressive}} and therefore only present when the \code{surrogate_learner} construction argument is not \code{NULL}:
}

\examples{
\donttest{
lgr::threshold("warn")

# Define the objective to optimize
# The 'budget' here simulates averaging 'b' samples from a noisy function
objective <- ObjectiveRFun$new(
  fun = function(xs) {
    z <- exp(-xs$x^2 - xs$y^2) + 2 * exp(-(2 - xs$x)^2 - (2 - xs$y)^2)
    z <- z + rnorm(1, sd = 1 / sqrt(xs$b))
    list(Obj = z)
  },
  domain = ps(x = p_dbl(-2, 4), y = p_dbl(-2, 4), b = p_int(1)),
  codomain = ps(Obj = p_dbl(tags = "maximize"))
)

search_space = objective$domain$search_space(list(
  x = to_tune(),
  y = to_tune(),
  b = to_tune(p_int(1, 2^10, logscale = TRUE, tags = "budget"))
))

# Get a new OptimInstance. Here we determine that the optimizatoin goes
# for 10 generations.
oi <- OptimInstanceSingleCrit$new(objective,
  search_space = search_space,
  terminator = trm("gens", generations = 10)
)

library("mlr3learners")
# use the 'regr.ranger' as surrogate.
# The following settings have 30 individuals in a batch, the 20 best
# of which survive, while 10 are sampled new.
# For this, 100 individuals are sampled randomly, and the top 10, according
# to the surrogate model, are used.
sumohb_opt <- opt("sumohb", surrogate_learner = mlr3::lrn("regr.ranger"),
  mu = 30, survival_fraction = 2/3,
  filter_rate_first = 100, filter_rate_per_sample = 0
)
# sumohb_opt$optimize performs SumoHB optimization and returns the optimum
sumohb_opt$optimize(oi)

#####
# Optimizing a Machine Learning Method
#####

# Note that this is a short example, aiming at clarity and short runtime.
# The settings are not optimal for hyperparameter tuning.

library("mlr3")
library("mlr3learners")
library("mlr3tuning")

# The Learner to optimize
learner = lrn("classif.xgboost")

# The hyperparameters to optimize
learner$param_set$values[c("eta", "booster")] = list(to_tune())
learner$param_set$values$nrounds = to_tune(p_int(1, 4, tags = "budget", logscale = TRUE))

# Get a TuningInstance
ti = TuningInstanceSingleCrit$new(
  task = tsk("iris"),
  learner = learner,
  resampling = rsmp("holdout"),
  measure = msr("classif.acc"),
  terminator = trm("gens", generations = 3)
)

sumohb_tune <- tnr("sumohb", surrogate_learner = lrn("regr.ranger"),
  mu = 20, survival_fraction = 0.5,
  filter_rate_first = 100, filter_rate_per_sample = 0
)
# sumohb_tune$optimize performs SumoHB optimization and returns the optimum
sumohb_tune$optimize(ti)

}
}
\seealso{
Other optimizers: 
\code{\link{OptimizerMies}}
}
\concept{optimizers}
\section{Super class}{
\code{\link[bbotk:Optimizer]{bbotk::Optimizer}} -> \code{OptimizerSumoHB}
}
\section{Active bindings}{
\if{html}{\out{<div class="r6-active-bindings">}}
\describe{
\item{\code{surrogate_learner}}{(\code{\link[mlr3:LearnerRegr]{mlr3::LearnerRegr}} | \code{NULL})\cr
Regression learner for the surrogate model filtering algorithm.}

\item{\code{param_set}}{(\code{\link[paradox:ParamSet]{ParamSet}})\cr
Configuration parameters of the optimization algorithm.}
}
\if{html}{\out{</div>}}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-new}{\code{OptimizerSumoHB$new()}}
\item \href{#method-clone}{\code{OptimizerSumoHB$clone()}}
}
}
\if{html}{
\out{<details open ><summary>Inherited methods</summary>}
\itemize{
\item \out{<span class="pkg-link" data-pkg="bbotk" data-topic="Optimizer" data-id="format">}\href{../../bbotk/html/Optimizer.html#method-format}{\code{bbotk::Optimizer$format()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="bbotk" data-topic="Optimizer" data-id="optimize">}\href{../../bbotk/html/Optimizer.html#method-optimize}{\code{bbotk::Optimizer$optimize()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="bbotk" data-topic="Optimizer" data-id="print">}\href{../../bbotk/html/Optimizer.html#method-print}{\code{bbotk::Optimizer$print()}}\out{</span>}
}
\out{</details>}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-new"></a>}}
\if{latex}{\out{\hypertarget{method-new}{}}}
\subsection{Method \code{new()}}{
Initialize the 'OptimizerSumoHB' object.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{OptimizerSumoHB$new(surrogate_learner = NULL)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{surrogate_learner}}{(\code{\link[mlr3:LearnerRegr]{mlr3::LearnerRegr}} | \code{NULL})\cr
Regression learner for the surrogate model filtering algorithm. May be \code{NULL}, in which case no surrogate model is used and individuals are sampled randomly.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-clone"></a>}}
\if{latex}{\out{\hypertarget{method-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{OptimizerSumoHB$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
\section{Super classes}{
\code{\link[mlr3tuning:Tuner]{mlr3tuning::Tuner}} -> \code{\link[mlr3tuning:TunerFromOptimizer]{mlr3tuning::TunerFromOptimizer}} -> \code{TunerSumoHB}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-new}{\code{TunerSumoHB$new()}}
\item \href{#method-clone}{\code{TunerSumoHB$clone()}}
}
}
\if{html}{
\out{<details open ><summary>Inherited methods</summary>}
\itemize{
\item \out{<span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format">}\href{../../mlr3tuning/html/Tuner.html#method-format}{\code{mlr3tuning::Tuner$format()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print">}\href{../../mlr3tuning/html/Tuner.html#method-print}{\code{mlr3tuning::Tuner$print()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="mlr3tuning" data-topic="TunerFromOptimizer" data-id="optimize">}\href{../../mlr3tuning/html/TunerFromOptimizer.html#method-optimize}{\code{mlr3tuning::TunerFromOptimizer$optimize()}}\out{</span>}
}
\out{</details>}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-new"></a>}}
\if{latex}{\out{\hypertarget{method-new}{}}}
\subsection{Method \code{new()}}{
Initialize the \code{TunerSumoHB} object.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TunerSumoHB$new(surrogate_learner = NULL)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{surrogate_learner}}{(\code{\link[mlr3:LearnerRegr]{mlr3::LearnerRegr}} | \code{NULL})}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-clone"></a>}}
\if{latex}{\out{\hypertarget{method-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TunerSumoHB$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
