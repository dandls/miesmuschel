% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/FiltorSurrogateProgressive.R
\name{FiltorSurrogateProgressive}
\alias{FiltorSurrogateProgressive}
\title{Progressive Surrogate Model Filtering}
\description{
Performs progressive surrogate model filtering.

A \emph{surrogate model} is a regression model, based on an \code{\link[mlr3:Learner]{mlr3::Learner}}, which predicts the approximate performance of newly sampled configurations
given the empirical performance of already evaluated configurations. If the optional \code{surrogate_learner} construction argument is given to \code{SumoHB},
then the surrogate model is used to propose points that have, according to the surrogate model, a relatively high chance of performing well.

The filtering is "progressive" in that successive values are filtered more agressively.
}
\section{Algorithm}{


Given the number \code{n_filter} of of individuals to sample, progressive surrogate model filtering proceeds as follows:
\enumerate{
\item Train the \code{surrogate_learner} \code{\link[mlr3:LearnerRegr]{LearnerRegr}} on the \code{known_values} and their \code{fitnesses}.
\item Take \code{filter_rate_first} configurations, predict their expected performance using the surrogate model, and put them
into a pool \code{P} of configurations to consider.
\item Take the individual that is optimal according to predicted performance, remove it from \code{P} and add it to solution set \code{S}.
\item If the number of solutions in \code{S} equals \code{n_filter}, quit.
\item Take the next \code{filter_rate_per_sample} configurations, predict their expected performance using the surrogate model, and add them to \code{P}.
\item Jump to 3.
}

(The algorithm presented here is optimized for clarity; the actual implementation does all the surrogate model prediction in one go, but is functionally
equivalent).

The \code{filter_rate_first} and \code{filter_rate_per_sample} configuration parameters of this algorithm determine how agressively the surrogate model is used to
filter out sampled configurations. If the filtering is agressive (\code{filter_rate_first} is large), then more "exploitation" at the cost of "exploration" is performed.
When \code{filter_rate_first} is small but \code{filter_rate_per_sample} is large, then successive individuals are filtered successively more agressively, potentially
leading to a tradeoff between "exploration" and "exploitation".

When \code{filter_rate_per_sample} is set to 0, then the method is equivalent to sampling the top \code{n_filter} individuals from \code{filter_rate_first}
sampled ones. When \code{filter_rate_per_sample} is 1 and \code{filter_rate_first} is 0, then the method is equivalent to random sampling.

\code{filter_rate_first} and \code{filter_rate_per_sample} may be fractional; the total number of individuals to select from when selecting the \code{i}th
individuals is always \code{round(filter_rate_first + (filter_rate_per_sample - 1) * (i - 1))}. However, \code{filter_rate_first} must
be at least 1, and \code{filter_rate_first + filter_rate_per_sample * (n_filter - 1)} must be at least \code{n_filter}.
}

\section{Configuration Parameters}{

\code{FiltorSurrogateProgressive}'s configuration parameters are the hyperparameters of the \code{surrogate_learner} \code{\link[mlr3:Learner]{Learner}}, as well as:
}

\seealso{
Other filtors: 
\code{\link{FiltorNull}},
\code{\link{Filtor}}
}
\concept{filtors}
\section{Super classes}{
\code{\link[miesmuschel:MiesOperator]{miesmuschel::MiesOperator}} -> \code{\link[miesmuschel:Filtor]{miesmuschel::Filtor}} -> \code{FiltorSurrogateProgressive}
}
\section{Active bindings}{
\if{html}{\out{<div class="r6-active-bindings">}}
\describe{
\item{\code{surrogate_learner}}{(\code{\link[mlr3:LearnerRegr]{mlr3::LearnerRegr}} | \code{NULL})\cr
Regression learner for the surrogate model filtering algorithm.}
}
\if{html}{\out{</div>}}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-new}{\code{FiltorSurrogateProgressive$new()}}
\item \href{#method-clone}{\code{FiltorSurrogateProgressive$clone()}}
}
}
\if{html}{
\out{<details open ><summary>Inherited methods</summary>}
\itemize{
\item \out{<span class="pkg-link" data-pkg="miesmuschel" data-topic="MiesOperator" data-id="operate">}\href{../../miesmuschel/html/MiesOperator.html#method-operate}{\code{miesmuschel::MiesOperator$operate()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="miesmuschel" data-topic="MiesOperator" data-id="prime">}\href{../../miesmuschel/html/MiesOperator.html#method-prime}{\code{miesmuschel::MiesOperator$prime()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="miesmuschel" data-topic="Filtor" data-id="needed_input">}\href{../../miesmuschel/html/Filtor.html#method-needed_input}{\code{miesmuschel::Filtor$needed_input()}}\out{</span>}
}
\out{</details>}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-new"></a>}}
\if{latex}{\out{\hypertarget{method-new}{}}}
\subsection{Method \code{new()}}{
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{FiltorSurrogateProgressive$new(surrogate_learner)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{surrogate_learner}}{(\code{\link[mlr3:LearnerRegr]{mlr3::LearnerRegr}} | \code{NULL})\cr
Regression learner for the surrogate model filtering algorithm.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-clone"></a>}}
\if{latex}{\out{\hypertarget{method-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{FiltorSurrogateProgressive$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
